.. _learn:

==================
Learning procedure
==================


Problem description
===================

We work with numerically generated training trajectories that we denote by 

.. math::

    \begin{align}
        \{(x_i,y_i^2,...,y_i^M)\}_{i=1,...,N}.
    \end{align}

To obtain an approximation of the Hamiltonian :math:`H`, we define a parametric model :math:`H_{\Theta}` and 
look for a :math:`\Theta` so that the trajectories generated by :math:`H_{\Theta}` resemble the given ones. 
:math:`H_{\Theta}` in principle can be any parametric function depending on the parameters :math:`\Theta`. 
In  our approach, :math:`\Theta` will collect a factor of the mass matrix and the weights of a neural network, 
as described below. We use some numerical one-step method :math:`\Psi_{X_{H_{\Theta}}}^{\Delta t}` to generate 
the trajectories

.. math::

    \begin{align}
        \hat{y}_i^j(\Theta) :=\Psi_{X_{H_{\Theta}}}^{\Delta t}(\hat{y}_i^{j-1}(\Theta)),\quad \hat{y}_i^1(\Theta) := x_i, \quad j=2,\dots,M, \; i=1,\dots,N.
    \end{align}

We then optimize a loss function measuring the distance between the given trajectories :math:`y^j_i` and the generated 
ones :math:`\hat{y}_i^j`, defined as

.. math::

    \begin{align}
        \mathcal{L}(\Theta):=\frac{1}{2n}\frac{1}{NM}\sum_{i=1}^N\mathcal{L}_i(\Theta) = \frac{1}{2n}\frac{1}{NM}\sum_{i=1}^N\sum_{j=1}^M \|\hat{y}_i^j(\Theta)- y_i^j\|^2,
    \end{align}

where :math:`\|\cdot\|` is the Euclidean metric of :math:`\mathbb{R}^{2n}`. This is implemented with the PyTorch 
:math:`\texttt{MSELoss}` loss function. Such a training procedure resembles the one of Recurrent Neural Networks (RNNs), 
as shown for the forward pass of a single training trajectory in the following figure.


.. figure:: /RNN_Diagram.png

   Figure 1. Forward pass of an input training trajectory :math:`(x_i,y_i^2,...,y_i^M)`. The picture highlights the resemblance to an unrolled version of a Recurrent Neural Network. The network outputs :math:`(\hat{y}_i^2,â€¦,\hat{y}_i^M)`.

Indeed, the weight sharing principle of RNNs is reproduced by the time steps in the numerical integrator which are all 
based on the same approximation of the Hamiltonian, and hence on the same weights :math:`\Theta`. In Algorithm WRITE ALGORITHM 
we report one training epoch for a batch of data points...