
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Algorithms &#8212; Learning_Hamiltonians 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Example" href="intro.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="algorithms">
<span id="learn"></span><h1>Algorithms<a class="headerlink" href="#algorithms" title="Permalink to this heading">¶</a></h1>
<section id="learning-procedure">
<h2>Learning procedure<a class="headerlink" href="#learning-procedure" title="Permalink to this heading">¶</a></h2>
<p>We work with numerically generated training trajectories that we denote by</p>
<div class="math notranslate nohighlight" id="equation-ttraj">
<span class="eqno">(1)<a class="headerlink" href="#equation-ttraj" title="Permalink to this equation">¶</a></span>\[\begin{align}
    \{(x_i,y_i^2,...,y_i^M)\}_{i=1,...,N}.
\end{align}\]</div>
<p>To obtain an approximation of the Hamiltonian <span class="math notranslate nohighlight">\(H\)</span>, we define a parametric model <span class="math notranslate nohighlight">\(H_{\Theta}\)</span> and
look for a <span class="math notranslate nohighlight">\(\Theta\)</span> so that the trajectories generated by <span class="math notranslate nohighlight">\(H_{\Theta}\)</span> resemble the given ones.
<span class="math notranslate nohighlight">\(H_{\Theta}\)</span> in principle can be any parametric function depending on the parameters <span class="math notranslate nohighlight">\(\Theta\)</span>.
In  our approach, <span class="math notranslate nohighlight">\(\Theta\)</span> will collect a factor of the mass matrix and the weights of a neural network,
as described below. We use some numerical one-step method <span class="math notranslate nohighlight">\(\Psi_{X_{H_{\Theta}}}^{\Delta t}\)</span> to generate
the trajectories</p>
<div class="math notranslate nohighlight" id="equation-ltraj">
<span class="eqno">(2)<a class="headerlink" href="#equation-ltraj" title="Permalink to this equation">¶</a></span>\[\begin{align}
    \hat{y}_i^j(\Theta) :=\Psi_{X_{H_{\Theta}}}^{\Delta t}(\hat{y}_i^{j-1}(\Theta)),\quad \hat{y}_i^1(\Theta) := x_i, \quad j=2,\dots,M, \; i=1,\dots,N.
\end{align}\]</div>
<p>We then optimize a loss function measuring the distance between the given trajectories <span class="math notranslate nohighlight">\(y^j_i\)</span> and the generated
ones <span class="math notranslate nohighlight">\(\hat{y}_i^j\)</span>, defined as</p>
<div class="math notranslate nohighlight" id="equation-loss">
<span class="eqno">(3)<a class="headerlink" href="#equation-loss" title="Permalink to this equation">¶</a></span>\[\begin{align}
    \mathcal{L}(\Theta):=\frac{1}{2n}\frac{1}{NM}\sum_{i=1}^N\mathcal{L}_i(\Theta) = \frac{1}{2n}\frac{1}{NM}\sum_{i=1}^N\sum_{j=1}^M \|\hat{y}_i^j(\Theta)- y_i^j\|^2,
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> is the Euclidean metric of <span class="math notranslate nohighlight">\(\mathbb{R}^{2n}\)</span>. This is implemented with the PyTorch
<span class="math notranslate nohighlight">\(\texttt{MSELoss}\)</span> loss function. Such a training procedure resembles the one of Recurrent Neural Networks (RNNs),
as shown for the forward pass of a single training trajectory in the following figure.</p>
<figure class="align-default" id="id1">
<img alt="_images/RNN_Diagram.png" src="_images/RNN_Diagram.png" />
<figcaption>
<p><span class="caption-text">Figure 1. Forward pass of an input training trajectory <span class="math notranslate nohighlight">\((x_i,y_i^2,...,y_i^M)\)</span>. The picture highlights the resemblance to an unrolled version of a Recurrent Neural Network. The network outputs <span class="math notranslate nohighlight">\((\hat{y}_i^2,…,\hat{y}_i^M)\)</span>.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Indeed, the weight sharing principle of RNNs is reproduced by the time steps in the numerical integrator which are all
based on the same approximation of the Hamiltonian, and hence on the same weights <span class="math notranslate nohighlight">\(\Theta\)</span>.</p>
</section>
<section id="architecture-of-the-network">
<h2>Architecture of the network<a class="headerlink" href="#architecture-of-the-network" title="Permalink to this heading">¶</a></h2>
<p>In this example, the role of the neural network is to model the Hamiltonian, i.e. a scalar function defined on the phase
space <span class="math notranslate nohighlight">\(\mathbb{R}^{2n}\)</span>. Thus, the starting and arrival spaces are fixed.</p>
<p>We leverage the form of the kinetic energy, where <span class="math notranslate nohighlight">\(M(q)\)</span> is modelled through a constant symmetric and positive
definite matrix with entries <span class="math notranslate nohighlight">\(m_{ij}\)</span>. Therefore, we aim at learning a constant matrix
<span class="math notranslate nohighlight">\(A\in\mathbb{R}^{k\times k}\)</span> and a vector <span class="math notranslate nohighlight">\(b\in\mathbb{R}^k\)</span> so that</p>
<div class="math notranslate nohighlight" id="equation-mmatr">
<span class="eqno">(4)<a class="headerlink" href="#equation-mmatr" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
    \begin{bmatrix}
    m_{11} &amp; ... &amp; m_{1k}\\
    m_{21} &amp; ... &amp; m_{2k}\\
    \vdots &amp; \vdots &amp; \vdots \\
    m_{k1} &amp; ... &amp; m_{kk}
    \end{bmatrix} \approx A^TA +
    \begin{bmatrix}
    \tilde{b}_{1} &amp; 0 &amp; ... &amp; 0 \\
    0 &amp; \tilde{b}_2 &amp; \ddots &amp; \vdots \\
    \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
    0 &amp; ... &amp; 0 &amp; \tilde{b}_k
    \end{bmatrix}
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{b}_i := \max{(0,b_i)}\)</span> are terms added to promote the positive definiteness of the right-hand side.
Notice that, in principle, the imposition of the positive (semi)definiteness of the matrix defining the kinetic energy
is not necessary, but it allows to get more interpretable results. Indeed, it is known that the kinetic energy should
define a metric on <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> and the assumption we are making guarantees such a property.  For the
potential energy, a possible modelling strategy is to work with standard feedforward neural networks, and hence to define</p>
<div class="math notranslate nohighlight" id="equation-pot">
<span class="eqno">(5)<a class="headerlink" href="#equation-pot" title="Permalink to this equation">¶</a></span>\[\begin{align}
    V(q) \approx V_{\theta}(q) = f_{\theta_m}\circ ...\circ f_{\theta_1}(q)
\end{align}\]</div>
<div class="math notranslate nohighlight" id="equation-pnn">
<span class="eqno">(6)<a class="headerlink" href="#equation-pnn" title="Permalink to this equation">¶</a></span>\[\begin{align}
    \theta_i = (W_i,b_i)\in\mathbb{R}^{n_i\times n_{i-1}}\times \mathbb{R}^{n_i},\;\theta:=[\theta_1,...,\theta_m],
\end{align}\]</div>
<div class="math notranslate nohighlight" id="equation-fnn">
<span class="eqno">(7)<a class="headerlink" href="#equation-fnn" title="Permalink to this equation">¶</a></span>\[\begin{align}
    f_{\theta_i}(u) := \Sigma(W_iu + b_i),\;\mathbb{R}^n\ni z\mapsto \Sigma(z) = [\sigma(z_1),...,\sigma(z_n)]\in\mathbb{R}^n,
\end{align}\]</div>
<p>for example with <span class="math notranslate nohighlight">\(\sigma(x) = \tanh(x)\)</span>. Therefore, we have that</p>
<div class="math notranslate nohighlight" id="equation-tpar">
<span class="eqno">(8)<a class="headerlink" href="#equation-tpar" title="Permalink to this equation">¶</a></span>\[\begin{align}
    \Theta = [A, \theta], \quad H(q,p) \approx H_{\Theta}(q,p) = K_A(p) + V_{\theta}(q).
\end{align}\]</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Learning_Hamiltonians</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Example</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#learning-procedure">Learning procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#architecture-of-the-network">Architecture of the network</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="intro.html" title="previous chapter">Example</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Elena Celledoni, Andrea Leone, Davide Murari, Brynjulf Owren.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/learn.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>